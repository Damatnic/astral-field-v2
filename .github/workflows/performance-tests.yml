name: Performance Tests

on:
  pull_request:
    branches: [main, develop]
  push:
    branches: [main, develop]
  schedule:
    # Run performance tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to test'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production
      test_suite:
        description: 'Test suite to run'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - api
          - frontend
          - load
          - memory

env:
  NODE_ENV: test

jobs:
  performance-test:
    name: Performance Testing
    runs-on: ubuntu-latest
    strategy:
      matrix:
        test-suite: 
          - api-performance
          - frontend-performance
          - database-performance
          - memory-usage
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Install performance testing tools
        run: |
          npm install -g lighthouse clinic autocannon
          sudo apt-get update
          sudo apt-get install -y curl jq

      - name: Set test environment
        run: |
          if [ "${{ github.event.inputs.environment }}" = "production" ]; then
            echo "BASE_URL=https://astralfield.com" >> $GITHUB_ENV
          else
            echo "BASE_URL=${{ secrets.STAGING_URL }}" >> $GITHUB_ENV
          fi

      - name: Run API Performance Tests
        if: matrix.test-suite == 'api-performance'
        run: |
          echo "üöÄ Running API Performance Tests"
          
          # Test critical API endpoints
          autocannon -c 10 -d 30 -j ${{ env.BASE_URL }}/api/health
          autocannon -c 10 -d 30 -j ${{ env.BASE_URL }}/api/leagues
          autocannon -c 10 -d 30 -j ${{ env.BASE_URL }}/api/players
          
          # Run custom API performance tests
          npm run perf:test:api -- --url=${{ env.BASE_URL }}
          
          # Save results
          mkdir -p performance-results
          mv autocannon-*.json performance-results/

      - name: Run Frontend Performance Tests
        if: matrix.test-suite == 'frontend-performance'
        run: |
          echo "üéØ Running Frontend Performance Tests"
          
          # Lighthouse performance audit
          lighthouse ${{ env.BASE_URL }} --output=json --output-path=./performance-results/lighthouse.json --chrome-flags="--headless --no-sandbox"
          
          # Core Web Vitals check
          npx web-vitals-cli ${{ env.BASE_URL }} --output=json > performance-results/web-vitals.json
          
          # Bundle size analysis
          npm run build
          npx bundlesize

      - name: Run Database Performance Tests
        if: matrix.test-suite == 'database-performance'
        run: |
          echo "üóÑÔ∏è Running Database Performance Tests"
          
          # Run database performance tests
          npm run perf:test:database
          
          # Test query performance
          npm run perf:test:queries

      - name: Run Memory Usage Tests
        if: matrix.test-suite == 'memory-usage'
        run: |
          echo "üß† Running Memory Usage Tests"
          
          # Memory leak detection
          npm run perf:test:memory
          
          # Heap analysis
          clinic doctor -- node scripts/performance/memory-analysis.js

      - name: Upload performance artifacts
        uses: actions/upload-artifact@v4
        with:
          name: performance-results-${{ matrix.test-suite }}
          path: performance-results/
          retention-days: 30

      - name: Comment PR with performance results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            try {
              // Read performance results
              let comment = '## üìä Performance Test Results\n\n';
              
              if ('${{ matrix.test-suite }}' === 'frontend-performance') {
                const lighthouse = JSON.parse(fs.readFileSync('performance-results/lighthouse.json'));
                const scores = lighthouse.lhr.categories;
                
                comment += '### Lighthouse Scores\n';
                comment += `- Performance: ${Math.round(scores.performance.score * 100)}/100\n`;
                comment += `- Accessibility: ${Math.round(scores.accessibility.score * 100)}/100\n`;
                comment += `- Best Practices: ${Math.round(scores['best-practices'].score * 100)}/100\n`;
                comment += `- SEO: ${Math.round(scores.seo.score * 100)}/100\n\n`;
              }
              
              if ('${{ matrix.test-suite }}' === 'api-performance') {
                comment += '### API Performance\n';
                comment += 'API load tests completed. Check artifacts for detailed results.\n\n';
              }
              
              comment += '### Test Environment\n';
              comment += `- Environment: ${{ github.event.inputs.environment || 'staging' }}\n`;
              comment += `- Commit: ${context.sha.substring(0, 7)}\n`;
              comment += `- Test Suite: ${{ matrix.test-suite }}\n`;
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            } catch (error) {
              console.log('Could not post performance results:', error.message);
            }

  load-test:
    name: Load Testing
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event.inputs.test_suite == 'load' || github.event.inputs.test_suite == 'all'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Install k6
        run: |
          sudo gpg -k
          sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
          echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install k6

      - name: Run comprehensive load tests
        run: |
          echo "üî• Running Load Tests"
          
          # Create k6 load test script
          cat > load-test.js << 'EOF'
          import http from 'k6/http';
          import { check, sleep } from 'k6';
          
          export let options = {
            stages: [
              { duration: '2m', target: 10 }, // Ramp up
              { duration: '5m', target: 50 }, // Stay at 50 users
              { duration: '2m', target: 100 }, // Ramp up to 100 users
              { duration: '5m', target: 100 }, // Stay at 100 users
              { duration: '2m', target: 0 }, // Ramp down
            ],
          };
          
          const BASE_URL = __ENV.BASE_URL || 'https://staging.astralfield.com';
          
          export default function() {
            let response = http.get(`${BASE_URL}/api/health`);
            check(response, {
              'health check status is 200': (r) => r.status === 200,
              'health check response time < 500ms': (r) => r.timings.duration < 500,
            });
            
            response = http.get(`${BASE_URL}/api/leagues`);
            check(response, {
              'leagues API status is 200': (r) => r.status === 200,
              'leagues API response time < 2000ms': (r) => r.timings.duration < 2000,
            });
            
            sleep(1);
          }
          EOF
          
          # Run load test
          BASE_URL=${{ env.BASE_URL || secrets.STAGING_URL }} k6 run --out json=performance-results/load-test.json load-test.js

      - name: Analyze load test results
        run: |
          echo "üìà Analyzing Load Test Results"
          
          # Extract key metrics
          jq '.metrics' performance-results/load-test.json > performance-results/metrics-summary.json
          
          # Check if performance thresholds are met
          FAILED_CHECKS=$(jq '.metrics.checks.values.fails' performance-results/load-test.json)
          if [ "$FAILED_CHECKS" -gt 0 ]; then
            echo "‚ùå Load test failed with $FAILED_CHECKS failed checks"
            exit 1
          fi
          
          echo "‚úÖ Load test passed all checks"

      - name: Upload load test results
        uses: actions/upload-artifact@v4
        with:
          name: load-test-results
          path: performance-results/
          retention-days: 30

  performance-regression-check:
    name: Performance Regression Check
    runs-on: ubuntu-latest
    needs: [performance-test]
    if: github.event_name == 'pull_request'
    steps:
      - name: Download current results
        uses: actions/download-artifact@v4
        with:
          name: performance-results-frontend-performance
          path: current-results/

      - name: Download baseline results
        continue-on-error: true
        uses: actions/download-artifact@v4
        with:
          name: performance-baseline
          path: baseline-results/

      - name: Compare performance metrics
        run: |
          echo "üîç Checking for Performance Regressions"
          
          if [ -f "baseline-results/lighthouse.json" ] && [ -f "current-results/lighthouse.json" ]; then
            # Compare Lighthouse scores
            BASELINE_PERF=$(jq '.lhr.categories.performance.score' baseline-results/lighthouse.json)
            CURRENT_PERF=$(jq '.lhr.categories.performance.score' current-results/lighthouse.json)
            
            REGRESSION=$(echo "$BASELINE_PERF - $CURRENT_PERF > 0.1" | bc -l)
            
            if [ "$REGRESSION" = "1" ]; then
              echo "‚ùå Performance regression detected!"
              echo "Baseline: $(echo "$BASELINE_PERF * 100" | bc -l)"
              echo "Current: $(echo "$CURRENT_PERF * 100" | bc -l)"
              exit 1
            else
              echo "‚úÖ No significant performance regression detected"
            fi
          else
            echo "‚ÑπÔ∏è No baseline data available for comparison"
          fi

      - name: Save current results as baseline
        if: github.ref == 'refs/heads/main'
        uses: actions/upload-artifact@v4
        with:
          name: performance-baseline
          path: current-results/
          retention-days: 90