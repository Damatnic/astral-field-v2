# Alertmanager configuration for AstralField v2.1
# Route alerts to appropriate channels with intelligent grouping

global:
  smtp_smarthost: 'localhost:587'
  smtp_from: 'alerts@astralfield.com'
  smtp_auth_username: 'alerts@astralfield.com'
  smtp_auth_password: 'your-email-password'

# Define notification templates
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Route configuration
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'default'
  
  routes:
    # Critical alerts go to all channels immediately
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 0s
      repeat_interval: 5m
    
    # Warning alerts during business hours
    - match:
        severity: warning
      receiver: 'warning-alerts'
      active_time_intervals:
        - business-hours
    
    # Database alerts to DBA team
    - match_re:
        alertname: 'Database.*'
      receiver: 'database-team'
    
    # Infrastructure alerts
    - match_re:
        alertname: '(HighCPUUsage|HighMemoryUsage|LowDiskSpace|ContainerRestarting)'
      receiver: 'infrastructure-team'

# Time intervals
time_intervals:
  - name: business-hours
    time_intervals:
      - times:
          - start_time: '09:00'
            end_time: '17:00'
        weekdays: ['monday:friday']
        location: 'America/New_York'

# Notification channels
receivers:
  - name: 'default'
    webhook_configs:
      - url: 'http://localhost:5001/webhook'
        send_resolved: true

  - name: 'critical-alerts'
    email_configs:
      - to: 'admin@astralfield.com'
        subject: 'üö® CRITICAL: {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Instance: {{ .Labels.instance }}
          Time: {{ .StartsAt }}
          {{ end }}
    
    slack_configs:
      - api_url: 'YOUR_SLACK_WEBHOOK_URL'
        channel: '#critical-alerts'
        color: 'danger'
        title: 'üö® Critical Alert: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *{{ .Annotations.summary }}*
          {{ .Annotations.description }}
          *Instance:* {{ .Labels.instance }}
          {{ end }}
        send_resolved: true
    
    webhook_configs:
      - url: 'http://localhost:5001/webhook/critical'
        send_resolved: true

  - name: 'warning-alerts'
    email_configs:
      - to: 'team@astralfield.com'
        subject: '‚ö†Ô∏è Warning: {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Instance: {{ .Labels.instance }}
          Time: {{ .StartsAt }}
          {{ end }}
    
    slack_configs:
      - api_url: 'YOUR_SLACK_WEBHOOK_URL'
        channel: '#alerts'
        color: 'warning'
        title: '‚ö†Ô∏è Warning: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *{{ .Annotations.summary }}*
          {{ .Annotations.description }}
          *Instance:* {{ .Labels.instance }}
          {{ end }}
        send_resolved: true

  - name: 'database-team'
    email_configs:
      - to: 'dba@astralfield.com'
        subject: 'üóÑÔ∏è Database Alert: {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          Database Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Instance: {{ .Labels.instance }}
          Time: {{ .StartsAt }}
          {{ end }}

  - name: 'infrastructure-team'
    email_configs:
      - to: 'infrastructure@astralfield.com'
        subject: 'üèóÔ∏è Infrastructure Alert: {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          Infrastructure Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Instance: {{ .Labels.instance }}
          Time: {{ .StartsAt }}
          {{ end }}

# Inhibition rules - prevent redundant alerts
inhibit_rules:
  # Inhibit warning if critical is firing
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'instance']
  
  # Inhibit any alert if application is down
  - source_match:
      alertname: 'ApplicationDown'
    target_match_re:
      alertname: '(HighResponseTime|HighErrorRate|DatabaseConnectionErrors)'
    equal: ['instance']